{"cells":[{"cell_type":"markdown","id":"89653b4312c21a54","metadata":{"collapsed":false,"id":"89653b4312c21a54"},"source":["## Connecting Colab to google drive\n","\n","Run the below cells to connect a google drive folder to your colab instance\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["%reload_ext autoreload\n","%autoreload 2\n"]},{"cell_type":"code","execution_count":2,"id":"6f2dcf433c720a3c","metadata":{"id":"6f2dcf433c720a3c"},"outputs":[],"source":["import torch\n","from torch import nn\n","import torchvision\n","from torch.utils.data import DataLoader\n","from network import RetinaNet\n","from detection_utils import compute_targets, get_detections, set_seed\n","from tensorboardX import SummaryWriter\n","from absl import app, flags\n","from tqdm import tqdm\n","import numpy as np\n","from dataset import CEPDOFDataset, Resizer, Normalizer, collater\n","from torchvision import transforms\n","import losses\n","import os"]},{"cell_type":"code","execution_count":3,"id":"WzP5qDx8cMAr","metadata":{"id":"WzP5qDx8cMAr"},"outputs":[],"source":["lr = 1e-3\n","momentum = 0.9\n","weight_decay = 1e-4\n","output_dir = \"runs\\\\retina-rotated-boxes-SmoothL1Loss-FixedLoss\\\\\"\n","batch_size = 1\n","seed = 2\n","max_iter = 120000\n","lr_step = [60000, 100000]"]},{"cell_type":"markdown","id":"78d4HANDD14K","metadata":{"id":"78d4HANDD14K"},"source":["## define model, Loss Function, dataloader, and learning rate scheduler\n"]},{"cell_type":"code","execution_count":4,"id":"AUHdheuCcDBU","metadata":{"id":"AUHdheuCcDBU"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\wyatt\\.conda\\envs\\MLCuda\\Lib\\site-packages\\torch\\overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n","  torch.has_cuda,\n","c:\\Users\\wyatt\\.conda\\envs\\MLCuda\\Lib\\site-packages\\torch\\overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n","  torch.has_cudnn,\n","c:\\Users\\wyatt\\.conda\\envs\\MLCuda\\Lib\\site-packages\\torch\\overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n","  torch.has_mps,\n","c:\\Users\\wyatt\\.conda\\envs\\MLCuda\\Lib\\site-packages\\torch\\overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n","  torch.has_mkldnn,\n","c:\\Users\\wyatt\\.conda\\envs\\MLCuda\\Lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3527.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","  0%|          | 0/120000 [00:00<?, ?it/s]"]}],"source":["torch.manual_seed(seed)\n","set_seed(seed)\n","dataset_train = CEPDOFDataset('train', seed=seed,\n","    transform=transforms.Compose([Normalizer(), Resizer()]))\n","dataset_val = CEPDOFDataset('val', seed=0,\n","    transform=transforms.Compose([Normalizer(), Resizer()]))\n","dataloader_train = DataLoader(dataset_train, num_workers=3, collate_fn=collater)\n","\n","model = RetinaNet(p67=True, fpn=True)\n","\n","## hard coded 2\n","num_classes = 2\n","device = torch.device('cuda:0')\n","model.to(device)\n","\n","\n","writer = SummaryWriter(output_dir, flush_secs=10)\n","optimizer = torch.optim.SGD(model.parameters(), lr=lr,\n","                            momentum=momentum,\n","                            weight_decay=weight_decay)\n","\n","milestones = [int(x) for x in lr_step]\n","\n","scheduler = torch.optim.lr_scheduler.MultiStepLR(\n","    optimizer, milestones=milestones, gamma=0.1)\n","\n","optimizer.zero_grad()\n","dataloader_iter = None\n","\n","cls_loss_np, bbox_loss_np, total_loss_np = [], [], []\n","\n","pbar = tqdm(range(max_iter))\n","\n","lossFunc = losses.LossFunc()\n","\n","\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n"]},{"cell_type":"markdown","id":"dXo4y8EBDtI0","metadata":{"id":"dXo4y8EBDtI0"},"source":["# RELOAD LATEST CHECKPOINT FROM output_dir FOLDER\n","\n","Useful for resuming training\n"]},{"cell_type":"code","execution_count":5,"id":"lEZR_fDov28Y","metadata":{"id":"lEZR_fDov28Y"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading model with checkpoint file runs\\retina-rotated-boxes-SmoothL1Loss-FixedLoss\\model_2000.pth\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/120000 [00:02<?, ?it/s]\n"]}],"source":["# load model weights from most recent file in checkpoint directory if it exists\n","import re\n","\n","\n","def load_latest_checkpoint(model, directory):\n","    \"\"\"\n","    Load the latest checkpoint from a directory.\n","\n","    :param directory: Directory containing the checkpoints of the form model{i}.pth\n","    :return: Loaded model checkpoint, iteration number\n","    \"\"\"\n","    all_files = os.listdir(directory)\n","\n","    # find all checkpoints of the form \"model{i}.pth\"\n","    checkpoint_files = [f for f in all_files if re.match(r\"model_\\d+\\.pth\", f)]\n","\n","    if not checkpoint_files:\n","        raise ValueError(\"No valid checkpoint files found in the directory.\")\n","\n","    # Extract the iteration number 'i' from each filename\n","    iterations = [\n","        int(re.search(r\"model_(\\d+)\\.pth\", f).group(1)) for f in checkpoint_files\n","    ]\n","\n","    # Find the file with the highest iteration number.\n","    max_iteration = max(iterations)\n","    latest_checkpoint_file = os.path.join(directory, f\"model_{max_iteration}.pth\")\n","    print(f\"Loading model with checkpoint file {latest_checkpoint_file}\")\n","\n","    # Load the model with latest_checkpoint file\n","    model.load_state_dict(torch.load(latest_checkpoint_file))\n","    return model, max_iteration\n","\n","\n","model, checkpoint_iteration = load_latest_checkpoint(model, output_dir)\n","\n","pbar = tqdm(range(max_iter))\n","pbar.update(checkpoint_iteration)"]},{"cell_type":"markdown","id":"eEIo4klIDfIK","metadata":{"id":"eEIo4klIDfIK"},"source":["### Prediction and Validate Functions\n","\n","(taken from predict.py)\n"]},{"cell_type":"code","execution_count":6,"id":"wvHti-hBzuo7","metadata":{"id":"wvHti-hBzuo7"},"outputs":[],"source":["from detection_utils import (\n","    compute_targets,\n","    compute_bbox_targets,\n","    get_detections,\n","    apply_bbox_deltas,\n","    nms,\n",")\n","import json\n","\n","\n","test_nms_thresh = 0.5\n","test_set = \"test\"\n","test_score_thresh = 0.25\n","test_det_thresh = 100\n","test_model_checkpoint = 85000\n","model_dir = \"runs\\\\retina-rotated-boxes-SmoothL1Loss-FixedLoss\\\\\"\n","\n","\n","def predict_image(image, image_id, resize_factor, model, cat_map):\n","    model.eval()\n","    results = []\n","\n","    with torch.no_grad():\n","        outs = model(image)\n","\n","        pred_clss, pred_bboxes, anchors = get_detections(outs)\n","        prob_clss = pred_clss.sigmoid()\n","        prob_clss = prob_clss[0, ...]\n","        pred_bboxes = pred_bboxes[0, ...]\n","        anchors = anchors[0, ...]\n","        out_bboxes = apply_bbox_deltas(anchors, pred_bboxes)\n","        out_bboxes[:, :4] = out_bboxes[:, :4] / resize_factor[0]\n","\n","        for j in range(prob_clss.shape[1]):\n","            prob_cls = prob_clss[:, j]\n","            out_bbox = out_bboxes + 0.0\n","            anchor = anchors + 0.0\n","            keep = prob_cls > test_score_thresh\n","            prob_cls = prob_cls[keep]\n","            out_bbox = out_bbox[keep, :]\n","            anchor = anchor[keep, :]\n","            keep = nms(anchor, prob_cls, test_nms_thresh)\n","            prob_cls = prob_cls[keep]\n","            out_bbox = out_bbox[keep, :]\n","            anchor = anchor[keep, :]\n","            keep = torch.argsort(prob_cls, descending=True)[:test_det_thresh]\n","            prob_cls = prob_cls[keep]\n","            out_bbox = out_bbox[keep, :]\n","            anchor = anchor[keep, :]\n","\n","            for a, p in zip(\n","                out_bbox.cpu().double().numpy(), prob_cls.cpu().double().numpy()\n","            ):\n","                res = {}\n","                res[\"image_id\"] = image_id[0]\n","                res[\"category_id\"] = 1 if p > 0.5 else 0\n","                res[\"bbox\"] = [a[0], a[1], a[2], a[3], a[4]] # outputs x1,y1,x2,y2,angle\n","                res[\"score\"] = p\n","                results.append(res)\n","    return results\n","\n","\n","def validate(dataset, dataloader, device, model, result_file_name, writer, iteration):\n","    model.eval()\n","\n","    results = []\n","    for i, (image, _, _, _, image_id, resize_factor) in enumerate(tqdm(dataloader)):\n","        image = image.to(device)\n","        results += predict_image(\n","            image, image_id, resize_factor, model, list(dataset.cat_map)\n","        )\n","    if len(results) > 0:\n","        json.dump(results, open(result_file_name, \"w\"))\n","        metrics, classes = dataset.evaluate(result_file_name)\n","        print_results(metrics, classes)\n","        log_results(writer, metrics, classes, iteration)\n","    else:\n","        print(\n","            f\"No detections above detection threshold of {test_score_thresh}, skipping evaluation.\"\n","        )\n","\n","def test(dataset, dataloader, device, model, result_file_name):\n","    model.eval()\n","    \n","    results = []\n","    for i, (image, _, _, _, image_id, resize_factor) in enumerate(tqdm(dataloader)):\n","        image = image.to(device)\n","        results += predict_image(image, image_id, resize_factor, model, \n","                                 list(dataset.cat_map))\n","    \n","    json.dump(results, open(result_file_name, 'w'))\n","\n","def print_results(metrics, classes):\n","    tt = \"\"\n","    print(f\"{tt:10s}\\tAP\\tAP50\\tAP75\\tAPs\\tAPm\\tAPl\")\n","    for i, (m, c) in enumerate(zip(metrics, classes)):\n","        print(\n","            f\"{c:>10s}\\t{m[0]:.3f}\\t{m[1]:.3f}\\t{m[2]:.3f}\\t{m[3]:.3f}\\t{m[4]:.3f}\\t{m[5]:.3f}\"\n","        )\n","\n","\n","def log_results(writer, metrics, classes, iteration):\n","    for i, (m, c) in enumerate(zip(metrics, classes)):\n","        if i == 0:\n","            writer.add_scalar(f\"AP\", m[0], iteration)\n","            writer.add_scalar(f\"AP50\", m[1], iteration)\n","            writer.add_scalar(f\"AP75\", m[2], iteration)\n","            for j, tag in enumerate([\"\", \"50\", \"75\", \"s\", \"m\", \"l\"]):\n","                writer.add_scalar(f\"bbox/AP{tag}\", m[j] * 100, iteration)\n","        else:\n","            writer.add_scalar(f\"AP/{c}\", m[0], iteration)\n","            writer.add_scalar(f\"AP50/{c}\", m[1], iteration)\n","            writer.add_scalar(f\"AP75/{c}\", m[2], iteration)\n","            writer.add_scalar(f\"bbox/AP-{c}\", m[0] * 100, iteration)"]},{"cell_type":"markdown","id":"tGvdSbHWEJry","metadata":{"id":"tGvdSbHWEJry"},"source":["## TRAINING LOOP\n","\n","\n"]},{"cell_type":"code","execution_count":7,"id":"4M5OAq-yzBiB","metadata":{"colab":{"background_save":true},"id":"4M5OAq-yzBiB"},"outputs":[{"name":"stderr","output_type":"stream","text":[]}],"source":["for i in pbar:\n","    if dataloader_iter is None or i % len(dataloader_iter) == 0:\n","        dataloader_iter = iter(dataloader_train)\n","\n","    image, cls, bbox, is_crowd, image_id, _ = next(dataloader_iter)\n","\n","    if len(bbox) == 0:\n","        continue\n","\n","    image = image.to(device)\n","    bbox = bbox.to(device)\n","    cls = cls.to(device)\n","\n","    outs = model(image)\n","    pred_clss, pred_bboxes, anchors = get_detections(outs)\n","    gt_clss, gt_bboxes = compute_targets(anchors, cls, bbox, is_crowd)\n","\n","    pred_clss = pred_clss.sigmoid()\n","    classification_loss, regression_loss = lossFunc(\n","        pred_clss, pred_bboxes, anchors, gt_clss, gt_bboxes\n","    )\n","    cls_loss = classification_loss.mean()\n","    bbox_loss = regression_loss.mean()\n","    total_loss = cls_loss + bbox_loss\n","\n","    if np.isnan(total_loss.item()):\n","        print(\"Loss went to NaN\")\n","        break\n","\n","    if np.isinf(total_loss.item()):\n","        print(\"Loss went to Inf\")\n","        break\n","\n","    total_loss.backward()\n","\n","    torch.nn.utils.clip_grad_norm_(model.parameters(),5)\n","\n","    optimizer.step()\n","    optimizer.zero_grad()\n","    scheduler.step()\n","\n","    # Some logging\n","    lr = scheduler.get_last_lr()[0]\n","    total_loss_np.append(total_loss.item())\n","    cls_loss_np.append(cls_loss.item())\n","    bbox_loss_np.append(bbox_loss.item())\n","\n","    if (i + 1) % 20 == 0:\n","        writer.add_scalar(\"loss_box_reg\", np.mean(bbox_loss_np), i + 1)\n","        writer.add_scalar(\"lr\", lr, i + 1)\n","        writer.add_scalar(\"loss_cls\", np.mean(cls_loss_np), i + 1)\n","        writer.add_scalar(\"total_loss\", np.mean(total_loss_np), i + 1)\n","        pbar.set_description(\n","            f\"{i+1} / {lr:5.6f} / {np.mean(cls_loss_np):5.3f} / {np.mean(bbox_loss_np):5.3f} / {np.mean(total_loss_np):5.3f}\"\n","        )\n","        cls_loss_np, bbox_loss_np, total_loss_np = [], [], []\n","\n","    if (i + 1) % 500 == 0:\n","        torch.save(model.state_dict(), f\"{output_dir}/model_{i+1}.pth\")\n","\n","    if (i + 1) % 2500 == 0 or (i + 1) == len(pbar):\n","        print(\"Validating...\")\n","        val_dataloader = DataLoader(dataset_val, num_workers=3, collate_fn=collater)\n","        result_file_name = f\"{output_dir}/results_{i+1}_val.json\"\n","        model.eval()\n","        validate(\n","            dataset_val, val_dataloader, device, model, result_file_name, writer, i + 1\n","        )\n","        model.train()\n","\n","torch.save(model.state_dict(), f\"{output_dir}/model_final.pth\")\n","\n","# Save prediction result on test set\n","dataset_test = CEPDOFDataset(\n","    \"test\", transform=transforms.Compose([Normalizer(), Resizer()])\n",")\n","test_dataloader = DataLoader(dataset_test, num_workers=1, collate_fn=collater)\n","result_file_name = f\"{output_dir}/results_{max_iter}_test.json\"\n","model.eval()\n","test(dataset_test, test_dataloader, device, model, result_file_name)"]},{"cell_type":"code","execution_count":null,"id":"fXTiaGgeBwsX","metadata":{"id":"fXTiaGgeBwsX"},"outputs":[],"source":["iteration_num = 10000\n","!python predict.py --test_model_checkpoint {iteration_num} --test_set test --model_dir {output_dir}"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":5}
